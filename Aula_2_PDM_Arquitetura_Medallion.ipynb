{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Arquitetura Medallion\n",
        "A Arquitetura Medallion é uma abordagem para construir um Data Warehouse ou Data Lake focada em qualidade, confiabilidade e acessibilidade dos dados. Ela organiza os dados em camadas com níveis crescentes de refinamento, estrutura e valor de negócio, similar a um processo de lapidação de um diamante bruto até uma joia polida e valiosa.\n",
        "\n",
        "As camadas da Arquitetura Medallion são:\n",
        "\n",
        "1. Bronze (Bruta):\n",
        "\n",
        "* Objetivo: armazenar os dados brutos, exatamente como foram capturados das fontes originais, sem qualquer transformação ou limpeza.\n",
        "* Formato: arquivos no formato original (CSV, JSON, XML, etc.), geralmente armazenados em um Data Lake (e.g., HDFS, S3).\n",
        "* Características:\n",
        "  * Dados brutos e não processados.\n",
        "  * Schema-on-read (o esquema é inferido quando os dados são lidos).\n",
        "  * Alta variedade e volume de dados.\n",
        "  * Baixa latência de ingestão (dados são disponibilizados rapidamente).\n",
        "  * Exemplo: logs de aplicação, feeds de dados de sensores, dados de redes sociais.\n",
        "\n",
        "2. Silver (Aprimorada):\n",
        "\n",
        "* Objetivo: limpar, transformar e enriquecer os dados brutos da camada Bronze.\n",
        "* Formato: dados estruturados em formatos como Parquet ou ORC, armazenados em um Data Lake ou Data Warehouse.\n",
        "* Características:\n",
        "  * Dados limpos, consistentes e com maior qualidade.\n",
        "  * Schema-on-write (o esquema é definido antes da escrita dos dados).\n",
        "  * Dados desduplicados e com valores nulos tratados.\n",
        "  * Adição de informações contextuais e enriquecimento dos dados.\n",
        "  * Exemplo: dados de clientes com informações demográficas unificadas e padronizadas, dados de vendas com informações de produtos e promoções.\n",
        "3. Gold (Refinada):\n",
        "\n",
        "* Objetivo: criar datasets agregados e otimizados para análises de negócios e tomada de decisão.\n",
        "* Formato: tabelas dimensionais e fatos, armazenadas em um Data Warehouse, Data Marts ou agregados pré-calculados para dashboards.\n",
        "* Características:\n",
        "  * Dados altamente estruturados e organizados para atender às necessidades específicas de negócio.\n",
        "  * Dados históricos e dados atuais consolidados.\n",
        "  * Dados otimizados para performance em consultas analíticas.\n",
        "  * Alta qualidade e confiabilidade dos dados.\n",
        "  * Exemplo: tabelas de dimensão de tempo, cliente e produto, tabelas de fatos de vendas, indicadores chave de performance (KPIs) pré-calculados.\n",
        "\n",
        "Benefícios da Arquitetura Medallion:\n",
        "\n",
        "* Escalabilidade e flexibilidade: permite lidar com grandes volumes e variedade de dados.\n",
        "* Qualidade e confiabilidade dos dados: assegura a qualidade dos dados através de um processo incremental de refinamento.\n",
        "* Agilidade: facilita a ingestão e processamento de novos dados.\n",
        "* Reutilização de dados: permite que os mesmos dados brutos sejam utilizados para diferentes propósitos.\n",
        "* Governança de dados: facilita a gestão e controle dos dados em cada camada.\n",
        "\n",
        "![medallion-architecture](https://miro.medium.com/v2/resize:fit:1400/1*O4ey_K0ZbsESf8na7OirJg.jpeg)\n"
      ],
      "metadata": {
        "id": "_NCGbNGg5jml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download dos arquivos\n",
        "\n",
        "Iremos baixar os arquivos de entrada:\n",
        "- clients.csv\n",
        "- vendas.csv\n",
        "\n",
        "Os dados em `vendas.csv` são relativos a vendas realizadas por atacadistas e distribuidores."
      ],
      "metadata": {
        "id": "wZNyqh8F8Yz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dados de clientes\n",
        "\n",
        "Iremos realizar o download de dados de clientes do link abaixo:"
      ],
      "metadata": {
        "id": "xNZitw1TEPTY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVJyy_fUNNyW"
      },
      "outputs": [],
      "source": [
        "!wget -O clients.csv https://www.dropbox.com/scl/fi/vd5hmlr7ghj2j5rx3w681/clients.csv?rlkey=rmcalhytfjm6nfklw7hhtykid&dl=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Se não funcionar o download acima, tente o link abaixo:**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "R-PhnPvyCnZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!gdown https://drive.google.com/uc?id=1SQn8nCPhdFXFOe2wZ9wn1exTAIdgo2QU"
      ],
      "metadata": {
        "id": "oInJIJszCryu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dados de vendas\n",
        "\n",
        "Iremos realizar o download dos dados de vendas presentes no arquivo `vendas.csv`:"
      ],
      "metadata": {
        "id": "Grs7LvgUEXbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O vendas.csv https://www.dropbox.com/scl/fi/y6h3do8rp9fhovunvj36c/vendas.csv?rlkey=m4yl4h8vzfyg5fq8vyb2sbd2x&st=nz4dme6m&dl=1"
      ],
      "metadata": {
        "id": "XKdikSZa7GNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Se não funcionar o download acima, tente o link abaixo:**"
      ],
      "metadata": {
        "id": "WpYgkXP3DLTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!gdown https://drive.google.com/uc?id=1ubiLTdjEdy8C86MdkW1HRyPOFZl4irT1"
      ],
      "metadata": {
        "id": "vAREVqLIDPXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisando dados de vendas\n",
        "Você está recebendo um conjunto de dados histórico de vendas de ERPs de várias empresas. Temos o histórico de vendas de várias empresas dentro do arquivo e, por isso, podemos ter períodos históricos diferentes de dados de vendas disponíveis.\n",
        "\n",
        "## Arquivos\n",
        "- **vendas.csv** - contém dados históricos de vendas até junho de 2022.\n",
        "- **clients.csv** - dados dos clientes que compraram o produto.\n",
        "\n",
        "## Campos do arquivo de vendas\n",
        "\n",
        "- *client_id*: id do cliente.\n",
        "- *items_count*: número de itens vendidos\n",
        "- *list_price*: preço do produto no catálogo da empresa.\n",
        "- *order_date*: data da venda.\n",
        "- *order_id*: id do pedido. Cada pedido pode conter vários produtos vendidos dentro dele.\n",
        "- *product_id*: id do produto vendido.\n",
        "- *sale_price*: preço vendido ao cliente.\n",
        "- *salesman_id*: id do vendedor.\n",
        "- *supplier_id*: id do fornecedor do produto. Por exemplo, a indústria fabricando do produto.\n",
        "- *company_id*: id da empresa. Temos dentro da base o histórico de vendas de várias empresas para clientes finais.\n",
        "- *product*: nome do produto.\n",
        "- *salesman*: nome do vendedor.\n",
        "- *supplier*: nome do fornecedor.\n",
        "- *client*: nome do cliente.\n",
        "\n",
        "\n",
        "## Campos do arquivo clients.csv\n",
        "- *client_id*: id do cliente.\n",
        "- *cnae_id*: CNAE do cliente que está realizando a compra.\n",
        "- *cod_city*: código da cidade no IBGE em que o cliente está localizado.\n",
        "- *cod_tract*: código do setor censitário no IBGE em que o cliente está localizado.\n",
        "- *cod_uf*: código da UF no IBGE em que o cliente está localizado.\n",
        "- *city*: cidade do cliente.\n",
        "- *state*: UF do cliente.\n",
        "- *client*: nome do cliente.\n",
        "- *company_id*: id da empresa. Temos dentro da base o histórico de vendas de várias empresas para clientes finais.\n"
      ],
      "metadata": {
        "id": "byQsilyGtLLJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ts2iqOfStVG"
      },
      "source": [
        "## Iniciando o PySpark\n",
        "\n",
        "Esta célula de código instala o Spark no ambiente de execução Colab. Aqui está uma explicação passo a passo:\n",
        "\n",
        "1. **`!apt-get install openjdk-11-jdk-headless -qq > /dev/null`**: este comando instala o OpenJDK 11 (versão headless, sem interface gráfica), que é um requisito para o Spark. O `-qq` suprime a saída e o `> /dev/null` redireciona a saída para o nada, tornando o processo mais silencioso.\n",
        "\n",
        "2. **`!wget -q https://dlcdn.apache.org/spark/spark-3.5.2/spark-3.5.2-bin-hadoop3.tgz`**: Este comando baixa o arquivo compactado do Spark 3.5.2 (construído para o Hadoop 3) do site oficial do Apache Spark. O `-q` suprime a saída de download.\n",
        "\n",
        "3. **`!tar xf spark-3.5.2-bin-hadoop3.tgz`**: Este comando extrai o arquivo compactado baixado do Spark, criando um diretório chamado `spark-3.5.2-bin-hadoop3`.\n",
        "\n",
        "4. **`!pip -q install findspark`**: Este comando instala a biblioteca `findspark` usando `pip`. Findspark é uma biblioteca Python que torna mais fácil configurar o Spark em um ambiente Python, principalmente no Colab. Ela define as variáveis de ambiente necessárias para que o Spark funcione corretamente.\n",
        "\n",
        "Após executar essas linhas, você terá o Spark instalado e pronto para ser usado em seu notebook Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek_RTaPXSwLp"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.2/spark-3.5.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.2-bin-hadoop3.tgz\n",
        "!pip -q install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defina as variáveis de ambiente do Spark:"
      ],
      "metadata": {
        "id": "duVQdGSXo1fS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qM3oCEa4Sx1g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.2-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código a seguir garante que o Spark seja configurado corretamente e esteja pronto para uso em seu ambiente Python.\n",
        "\n",
        "* **`findspark.init()`**: executa a função `init()` do módulo `findspark`. Esta função:\n",
        "    * Localiza a instalação do Spark em seu sistema.\n",
        "    * Configura as variáveis de ambiente necessárias para que o Python possa interagir com o Spark. Isso permite que o driver Python (seu código Python) se comunique com o executor Spark (o código que realmente processa os dados).\n"
      ],
      "metadata": {
        "id": "QV6xIUNwo2KZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgkpQMC6S0SE"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depois de executar a célula anterior, você poderá importar e usar as bibliotecas Spark como `pyspark.sql.SparkSession` para criar uma sessão Spark e começar a trabalhar com dados."
      ],
      "metadata": {
        "id": "WMUFJ_Bto5BJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAhdigi7S04j"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder.appName('Aula 1').master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRw-e1pRT9qD"
      },
      "source": [
        "# Primeira etapa: carregar os arquivos\n",
        "\n",
        "Nessa etapa você deve carregar os quatro arquivos abaixos, utilizando o **Spark**\n",
        "\n",
        "**Dicas:**\n",
        "\n",
        "- Separador dos arquivos é , (vírgula)\n",
        "- Os arquivos possuem cabeçalho"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clients_df = spark.read.csv(\"clients.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "gSa8umScM6NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients_df.show()"
      ],
      "metadata": {
        "id": "C2eQZ6FmNvqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients_df.printSchema()"
      ],
      "metadata": {
        "id": "eH1zh5moNerl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzsaH61tUH6b"
      },
      "outputs": [],
      "source": [
        "clients_schema = \"city string, client_id string, cnae_id string, \\\n",
        "cod_city integer, cod_tract long, cod_uf integer, state string, client string, \\\n",
        " company_id integer\"\n",
        "clients_df = spark.read.csv(\"clients.csv\", header=True, schema=clients_schema, mode=\"DROPMALFORMED\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vendas_df = spark.read.csv(\"vendas.csv\",header=True)"
      ],
      "metadata": {
        "id": "U7yMcAkXXlKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFpp2lmlUT8A"
      },
      "outputs": [],
      "source": [
        "vendas_schema = \"client_id string, items_count integer, list_price float, \\\n",
        "order_date date, order_id integer, product_id string, sale_price float, \\\n",
        "salesman_id string, supplier_id string, company_id integer, product string, \\\n",
        "salesman string, supplier string, client string\"\n",
        "vendas_df = spark.read.csv(\"vendas.csv\",header=True, schema=vendas_schema)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vendas_df.printSchema()"
      ],
      "metadata": {
        "id": "2CXctYolWiYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbeJjvp3X7jQ"
      },
      "outputs": [],
      "source": [
        "clients_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vendas_df.show(1)"
      ],
      "metadata": {
        "id": "QyeGXccNX6Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Pandas\n",
        "\n",
        "### O que é Spark Pandas?\n",
        "\n",
        "Spark Pandas é uma biblioteca que fornece uma interface similar ao Pandas para trabalhar com dados em clusters Apache Spark. Isso significa que você pode usar as mesmas funções e métodos do Pandas, mas com a capacidade de processar datasets imensos distribuídos em vários nós.\n",
        "\n",
        "### Por que usar Spark Pandas?\n",
        "\n",
        "Para cientistas de dados, o Spark Pandas oferece diversas vantagens:\n",
        "\n",
        "- Escalabilidade: processa conjuntos de dados enormes com rapidez e eficiência, utilizando a computação distribuída do Spark.\n",
        "- Familiaridade: permite utilizar a linguagem e as funções do Pandas, que você já conhece, para análise de dados em grande escala.\n",
        "- Performance: aproveita as otimizações do Spark para acelerar tarefas como leitura, transformação e agregação de dados.\n",
        "- Integração: funciona perfeitamente com outros componentes do ecossistema Spark, como Spark SQL e MLlib."
      ],
      "metadata": {
        "id": "D3KMk5UC7Ey3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos fazer a mesma operação de leitura de dados de clientes e vendas com **Spark Pandas** (vide [documentação](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html)).\n",
        "\n",
        "Os clientes vamos armazenar no Dataframe `clients_pdf` e os dados de vendas em `vendas_pdf`. A leitura do csv é semelhante ao Pandas com o método `read_csv`."
      ],
      "metadata": {
        "id": "Sx-MIEKB7K_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.pandas as ps"
      ],
      "metadata": {
        "id": "UkjV-kl37VMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients_pdf = ps.read_csv('clients.csv', names=clients_schema, header=0)"
      ],
      "metadata": {
        "id": "JT6o5V7_7THu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients_pdf.dtypes"
      ],
      "metadata": {
        "id": "b6daEk7D8f65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vendas_pdf = ps.read_csv(\"vendas.csv\", names=vendas_schema, header=0)"
      ],
      "metadata": {
        "id": "MlXh5Q9_6wkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vendas_pdf.head(5)"
      ],
      "metadata": {
        "id": "4unpRqVW8W8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Consultas nas bases de dados"
      ],
      "metadata": {
        "id": "LVOKNJ9dEnGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clients_df.createOrReplaceTempView(\"clients\")"
      ],
      "metadata": {
        "id": "BLSGXb8Yf5hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = \"\"\"\n",
        "select state, client_id\n",
        "from clients\n",
        "limit 10\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HXTDxC57hNNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(sql).show()"
      ],
      "metadata": {
        "id": "DcbcHiFBhgKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = \"\"\"\n",
        "select state, count(*) as contagem\n",
        "from clients\n",
        "where cnae_id is not null\n",
        "group by state\n",
        "order by contagem asc\n",
        "limit 10\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "798zHub-fiuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(sql).show()"
      ],
      "metadata": {
        "id": "VVzIGEYAgtOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients_pdf[ clients_pdf.cnae_id.isnull() == False]['state'].value_counts().sort_values(\n",
        "    ascending=False).head(10).to_frame()\n"
      ],
      "metadata": {
        "id": "zcPLb47vk1ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients_df.filter(\"cnae_id is not null\")\\\n",
        ".groupBy(\"state\").count()\\\n",
        ".sort(desc(\"count\"))\\\n",
        ".limit(10) \\\n",
        ".show()"
      ],
      "metadata": {
        "id": "ARFAfPKge-sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients_pdf[clients_pdf['state'] == 'GO'].head(3)"
      ],
      "metadata": {
        "id": "3dnLQisPEq0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients_df.filter(clients_df['state'] == 'GO').show(3)"
      ],
      "metadata": {
        "id": "HBsNd0RNEu3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Camadas da Arquitetura Medallion\n",
        "\n",
        "## Bronze (Dados Brutos):\n",
        "\n",
        "* Carregar os arquivos CSV vendas.csv e clientes.csv como DataFrames Spark.\n",
        "* Criar tabelas brutas na camada Bronze, armazenando os dados brutos sem alterações.\n",
        "\n",
        "## Silver (Dados Limpos e Enriquecidos):\n",
        "\n",
        "* Ler os dados das tabelas da camada Bronze.\n",
        "* Limpeza de dados:\n",
        "  * Remover duplicatas.\n",
        "  * Tratar valores ausentes, utilizando estratégias adequadas para cada coluna (ex: preenchimento com valores padrão, médias, etc.).\n",
        "  * Converter tipos de dados para os formatos corretos (ex: datas, números).\n",
        "* Enriquecimento de dados:\n",
        "  * Juntar as tabelas vendas e clientes utilizando a chave client_id.\n",
        "  * Criar novas colunas com informações relevantes, como:\n",
        "  `valor_total`: multiplicar `items_count` por `sale_price`.\n",
        "  * Ano da venda (extraído da coluna `order_date`).\n",
        "  * Mês da venda (extraído da coluna `order_date`).\n",
        "  * Salvar os dados enriquecidos e limpos como tabelas na camada Silver.\n",
        "\n",
        "## Gold (Dados Agregados):\n",
        "\n",
        "* Ler os dados da camada Silver.\n",
        "* Criar agregações para diferentes níveis de análise:\n",
        "  * Total de vendas por dia.\n",
        "  * Total de vendas por mês.\n",
        "  * Total de vendas por cliente.\n",
        "  * Total de vendas por produto.\n",
        "  * Total de vendas por vendedor.\n",
        "\n",
        "Armazenar os dados agregados em tabelas na camada Gold, otimizadas para consultas analíticas."
      ],
      "metadata": {
        "id": "50S6eMBa59UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Junção da base de dados de clientes e vendas"
      ],
      "metadata": {
        "id": "MOWVv9RLWmZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = vendas_df.join(clients_df, \"client_id\", \"left\")"
      ],
      "metadata": {
        "id": "bxuQlg7RWpnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vendas_df.show(1)"
      ],
      "metadata": {
        "id": "qT6ZLHMBo7C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df.show(1)"
      ],
      "metadata": {
        "id": "yOE8lATIW3jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_pdf = vendas_pdf.merge(clients_pdf, on='client_id', how='left')"
      ],
      "metadata": {
        "id": "dhSaw9DjWsh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(joined_pdf)"
      ],
      "metadata": {
        "id": "2qXMR5uVpMgT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}